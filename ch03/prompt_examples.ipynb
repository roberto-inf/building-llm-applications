{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c71df8f9-6782-4d7f-ae63-dc2d8edebdbd",
   "metadata": {},
   "source": [
    "# Chapter 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2fe42d-a428-4020-a2f7-d838b7c1172f",
   "metadata": {},
   "source": [
    "# OpenAI API prompt examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5688cd-b2ae-47e8-bf5b-4d4222216c0e",
   "metadata": {},
   "source": [
    "## Minimal prompt execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a00a004-c42b-49c6-8a67-4446a9f51d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your OPENAI_API_KEY ········\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import getpass\n",
    "\n",
    "OPENAI_API_KEY = getpass.getpass('Enter your OPENAI_API_KEY') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efe5a5c2-a624-4cb5-ba3f-a083fceb5a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06226bd6-59cf-47ef-aa1a-7219e473ae4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-9aQPGN5X7REwNUF5YyMwvgJYVVpTs', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='\"Stay alert and always verify before clicking on any links or sharing personal information to protect yourself from phishing attacks.\"', role='assistant', function_call=None, tool_calls=None))], created=1718468342, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=22, prompt_tokens=32, total_tokens=54))\n"
     ]
    }
   ],
   "source": [
    "prompt_input = \"\"\"Write a coincise message to remind users to be vigilant about phishing attacks.\"\"\"\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt_input}\n",
    "  ],\n",
    "  temperature= 0.7,\n",
    "  max_tokens= 400  \n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68461472-d9a5-4d07-8e38-ad836c24ac86",
   "metadata": {},
   "source": [
    "# Running prompts with LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4df874-4329-4a2a-a57c-d199800ab42c",
   "metadata": {},
   "source": [
    "## Basic prompt with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e46fe91-9c69-41de-88f7-424bb3648cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY,\n",
    "                 model_name=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65bf714e-35a8-446a-acaf-58be718823f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stay alert to potential phishing attacks. Be cautious of emails or messages asking for personal information or login credentials. If in doubt, contact the organization directly to verify the request. Protect yourself from cyber threats.\n"
     ]
    }
   ],
   "source": [
    "prompt_input = \"\"\"Write a coincise message to remind users to be vigilant about phishing attacks.\"\"\"\n",
    "\n",
    "response = llm.invoke(prompt_input)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3117386-5c78-4eef-a911-c02957aeddc9",
   "metadata": {},
   "source": [
    "# Few shot prompt with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c893189-e23d-478a-a9ed-5a671c697a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY,\n",
    "                 model_name=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "581cb1f7-1e61-45c6-8132-0c97715c4a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 // not divisible by 5, not divisible by 7 // None\n",
      "4 // not divisible by 5, not divisible by 7 // None\n",
      "5 // divisible by 5, not divisible by 7 // Abra\n",
      "7 // not divisible by 5, divisible by 7 // Kadabra\n",
      "8 // not divisible by 5, not divisible by 7 // None\n",
      "10 // divisible by 5, not divisible by 7 // Abra\n",
      "11 // not divisible by 5, not divisible by 7 // None\n",
      "13 // not divisible by 5, not divisible by 7 // None\n",
      "35 // divisible by 5, divisible by 7 // Abra Kadabra\n"
     ]
    }
   ],
   "source": [
    "prompt_input = \"\"\"Classify the following numbers as Abra, Kadabra or Abra Kadabra:\n",
    "\n",
    "3, 4, 5, 7, 8, 10, 11, 13, 35\n",
    "\n",
    "Examples: \n",
    "6 // not divisible by 5, not divisible by 7 // None\n",
    "15 // divisible by 5, not divisible by 7 // Abra\n",
    "12 // not divisible by 5, not divisible by 7 // None\n",
    "21 // not divisible by 5, divisible by 7 // Kadabra\n",
    "70 // divisible by 5, divisible by 7 // Abra Kadabra\n",
    "\"\"\"\n",
    "\n",
    "response = llm.invoke(prompt_input)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2820de-7028-4fbb-8948-088327a450fc",
   "metadata": {},
   "source": [
    "## Using Langchain's FewShotProm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "982749c6-f3bf-4449-864e-7073b98766b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 \\ Abra\n",
      "4 \\ Abra\n",
      "5 \\ Abra\n",
      "7 \\ Kadabra\n",
      "8 \\ Abra\n",
      "10 \\ Abra\n",
      "11 \\ Abra\n",
      "13 \\ Abra\n",
      "35 \\ Abra Kadabra\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "\n",
    "examples = [\n",
    "  {\n",
    "      \"number\": 6,\n",
    "      \"reasoning\": \"not divisible by 5 nor by 7\",\n",
    "      \"result\": \"None\"\n",
    "  },\n",
    "  {\n",
    "      \"number\": 15,\n",
    "      \"reasoning\": \"divisible by 5 but not by 7\",\n",
    "      \"result\": \"Abra\"\n",
    "  },\n",
    "  {\n",
    "      \"number\": 12,\n",
    "      \"reasoning\": \"not divisible by 5 nor by 7\",\n",
    "      \"result\": \"None\"\n",
    "  },\n",
    "  {\n",
    "      \"number\": 21,\n",
    "      \"reasoning\": \"divisible by 7 but not by 5\",\n",
    "      \"result\": \"Kadabra\"\n",
    "  },\n",
    "  {\n",
    "      \"number\": 70,\n",
    "      \"reasoning\": \"divisible by 5 and by 7\",\n",
    "      \"result\": \"Abra Kadabra\"\n",
    "  } ]\n",
    "\n",
    "example_prompt = PromptTemplate(input_variables=[\"number\", \"reasoning\", \"result\"], template=\"{number} \\\\ {reasoning} \\\\ {result}\")\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Classify the following numbers as Abra, Kadabra or Abra Kadabra: {comma_delimited_input_numbers}\",\n",
    "    input_variables=[\"comma_delimited_input_numbers\"]\n",
    ")\n",
    "\n",
    "prompt_input = few_shot_prompt.format(comma_delimited_input_numbers=\"3, 4, 5, 7, 8, 10, 11, 13, 35.\")\n",
    "\n",
    "response = llm.invoke(prompt_input)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68fa835-9282-4620-afe9-533841728c4c",
   "metadata": {},
   "source": [
    "# Prompt templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803951c2-4aa9-487c-915a-4af2f8195d78",
   "metadata": {},
   "source": [
    "## Prompt template - implementing it with a Python function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1faf059-7928-45ea-8af5-6c28d4ed8bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_summary_prompt(text, num_words, tone):\n",
    "    return f'You are an experienced copywriter. Write a {num_words} words summary the the following text, using a {tone} tone: {text}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ed7b1779-5c2e-4eb7-9d77-01d9683e52d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Aqueduct of Segovia, a well-preserved Roman marvel, channeled water to the city's fountains and baths. UNESCO World Heritage site.\n"
     ]
    }
   ],
   "source": [
    "segovia_aqueduct_text = \"The Aqueduct of Segovia (Spanish: Acueducto de Segovia) is a Roman aqueduct in Segovia, Spain. It was built around the first century AD to channel water from springs in the mountains 17 kilometres (11 mi) away to the city's fountains, public baths and private houses, and was in use until 1973. Its elevated section, with its complete arcade of 167 arches, is one of the best-preserved Roman aqueduct bridges and the foremost symbol of Segovia, as evidenced by its presence on the city's coat of arms. The Old Town of Segovia and the aqueduct, were declared a UNESCO World Heritage Site in 1985. As the aqueduct lacks a legible inscription (one was apparently located in the structure's attic, or top portion[citation needed]), the date of construction cannot be definitively determined. The general date of the Aqueduct's construction was long a mystery, although it was thought to have been during the 1st century AD, during the reigns of the Emperors Domitian, Nerva, and Trajan. At the end of the 20th century, Géza Alföldy deciphered the text on the dedication plaque by studying the anchors that held the now missing bronze letters in place. He determined that Emperor Domitian (AD 81–96) ordered its construction[1] and the year 98 AD was proposed as the most likely date of completion.[2] However, in 2016 archeological evidence was published which points to a slightly later date, after 112 AD, during the government of Trajan or in the beginning of the government of emperor Hadrian, from 117 AD.\"\n",
    "\n",
    "input_prompt = generate_text_summary_prompt(text=segovia_aqueduct_text, num_words=20, tone=\"knowledgeable and engaging\")\n",
    "\n",
    "response = llm.invoke(input_prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6279ce8c-417f-4cf3-a4c1-912a8322ce98",
   "metadata": {},
   "source": [
    "## Prompt template - using LangChain's ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f26d82e6-8d4c-4ef1-b4ef-10d123e4ab66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\"You are an experienced copywriter. Write a {num_words} words summary the the following text, using a {tone} tone: {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "05ffc67f-7381-4664-a752-df97c0e9faed",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prompt_template.format(text=segovia_aqueduct_text, num_words=20, tone=\"knowledgeable and engaging\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1b95f48e-a0d6-450d-aa3f-356bfaf733f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Aqueduct of Segovia, a Roman marvel built in the 1st century AD, channels water to Segovia's fountains and baths.\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ab2b4a-4db0-4e67-a2a9-ed8206236cf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
